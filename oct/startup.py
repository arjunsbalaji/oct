# AUTOGENERATED! DO NOT EDIT! File to edit: 01_startup.ipynb (unless otherwise specified).

__all__ = ['saveDictToConfigJSON', 'loadConfigJSONToDict', 'DeepConfig', 'binify', 'SegCustomLabelList',
           'SegCustomItemList', 'sens', 'spec', 'acc', 'Dice_Loss', 'my_Dice_Loss', 'saveResultsJSON',
           'save_all_results', 'clean_tensor_lists', 'MLFlowTracker']

# Cell
from fastai.vision import *
import mlflow
import model
import pandas as pd
import mlflow.pytorch as MYPY

# Cell
def saveDictToConfigJSON(dictiontary, name):
    with open('/workspace/oct_ca_seg/oct/configs/'+ name, 'w') as file:
        json.dump(dictiontary, file)

# Cell
def loadConfigJSONToDict(fn):
    with open('/workspace/oct_ca_seg/oct/configs/'+ fn, 'r') as file:
        config = json.load(file)
        return config

# Cell
class DeepConfig():
    def __init__(self, config_dict):

        self.config_dict = config_dict

        for k,v in self.config_dict.items():
            setattr(self, k, DeepConfig(v) if isinstance(v, dict) else v)

# Cell
binify = lambda x : x.point(lambda p: float(p>100.))#.point(lambda p: float(p))

# Cell
class SegCustomLabelList(SegmentationLabelList):
    def open(self, fn): return open_mask(fn, after_open=binify, convert_mode='L')

# Cell
class SegCustomItemList(SegmentationItemList):
    _label_cls = SegCustomLabelList

# Cell
def sens(c, l):
    n_targs=l.size()[0]
    c = c.argmax(dim=1).view(n_targs,-1).float()
    l = l.view(n_targs, -1).float()
    inter = torch.sum(c*l, dim=(1))
    union = torch.sum(c, dim=(1)) + torch.sum(l, dim=1) - inter
    return (inter/union).mean()

def spec(c, l):
    n_targs=l.size()[0]
    c = c.argmax(dim=1).view(n_targs,-1).float()
    l = l.view(n_targs, -1).float()
    c=1-c
    l=1-l
    inter = torch.sum(c*l, dim=(1))
    union = torch.sum(c, dim=(1)) + torch.sum(l, dim=1) - inter
    return (inter/union).mean()

'''def acc(c, l):
    n_targs=l.size()[0]
    c = c.argmax(dim=1).view(n_targs,-1)
    l = l.view(n_targs, -1).float()
    return torch.sum(c,dim=(1)) / l.size()[-1]'''

def acc(c, l):
    n_targs=l.size()[0]
    c = c.argmax(dim=1).view(n_targs,-1)
    l = l.argmax(dim=1).view(n_targs,-1)
    c = torch.sum(torch.eq(c,l).float(),dim=1)
    return (c/l.size()[-1]).mean()


class Dice_Loss(torch.nn.Module):
    """This is a custom Dice Similarity Coefficient loss function that we use
    to the accuracy of the segmentation. it is defined as ;
    DSC = 2 * (pred /intersect label) / (pred /union label) for the losss we use
    1- DSC so gradient descent leads to better outputs."""
    def __init__(self, weight=None, size_average=False):
        super(Dice_Loss, self).__init__()

    def forward(self, pred, label):
        label = label.float()
        smooth = 1.              #helps with backprop
        intersection = torch.sum(pred * label)
        union = torch.sum(pred) + torch.sum(label)
        loss = (2. * intersection + smooth) / (union + smooth)
        #return 1-loss because we want to minimise dissimilarity
        return 1 - (loss)

def my_Dice_Loss(pred, label):
        pred = torch.argmax(pred, dim=0,keepdim=True)
        label = label.float()
        smooth = 1.              #helps with backprop
        intersection = torch.sum(pred * label)
        union = torch.sum(pred) + torch.sum(label)
        loss = (2. * intersection + smooth) / (union + smooth)
        #return 1-loss because we want to minimise dissimilarity
        return 1 - (loss)

# Cell
clean_tensor_lists = lambda l : [x.item() for x in l]

def saveResultsJSON(json, run_dir, name): #name = exp_name+'item name+ .json'
    with open(run_dir +'/'+ name, 'w') as file:
        file.write(json)


def save_all_results(learner, run_dir, exp_name): #only call this function after training
    train_losses = pd.DataFrame(clean_tensor_lists(learner.recorder.losses)).to_json()
    valid_losses = pd.DataFrame(learner.recorder.val_losses).to_json()
    metrics = np.array([clean_tensor_lists(x) for x in learner.recorder.metrics])
    metric_names = learner.recorder.metrics_names
    metrics = pd.DataFrame(metrics, columns=metric_names).to_json()

    saveResultsJSON(train_losses, run_dir,'trainL.json')
    saveResultsJSON(valid_losses, run_dir,'validL.json')
    saveResultsJSON(metrics, run_dir,'metrics.json')


# Cell
## Tracking Class
from mlflow.tracking import MlflowClient
from mlflow.entities.run import Run

class MLFlowTracker(LearnerCallback):
    "A `TrackerCallback` that tracks the loss and metrics into MLFlow"
    def __init__(self, learn:Learner, exp_name: str, params: dict, nb_path: str, log_model: bool,uri: str = "http://localhost:5000"):
        super().__init__(learn)
        self.learn = learn
        self.exp_name = exp_name
        self.params = params
        self.nb_path = nb_path
        self.log_model = log_model
        self.uri = uri
        self.metrics_names = ['train_loss', 'valid_loss'] + [o.__name__ for o in learn.metrics]

    def on_train_begin(self, **kwargs: Any) -> None:
        "Prepare MLflow experiment and log params"
        self.client = mlflow.tracking.MlflowClient(self.uri)
        exp = self.client.get_experiment_by_name(self.exp_name)
        if exp is None:
            self.exp_id = self.client.create_experiment(self.exp_name)
        else:
            self.exp_id = exp.experiment_id
        run = self.client.create_run(experiment_id=self.exp_id)
        self.artifact_uri = run.info.artifact_uri
        self.run = run.info.run_uuid
        for k,v in self.params.items():
            self.client.log_param(run_id=self.run, key=k, value=v)

    def on_epoch_end(self, epoch, **kwargs:Any)->None:
        "Send loss and metrics values to MLFlow after each epoch"
        if kwargs['smooth_loss'] is None or kwargs["last_metrics"] is None:
            return
        metrics = [kwargs['smooth_loss']] + kwargs["last_metrics"]
        for name, val in zip(self.metrics_names, metrics):
            self.client.log_metric(self.run, name, np.float(val))

    def on_train_end(self, **kwargs: Any) -> None:
        "Store the notebook and stop run"
        #self.client.log_artifact(run_id=self.run, local_path=self.nb_path)
        #if self.log_model: MYPY.log_model(self.learn.model, self.exp_name)
        self.client.set_terminated(run_id=self.run)