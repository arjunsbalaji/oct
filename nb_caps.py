
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/caps_model.ipynb

from fastai.vision import *
from fastai.metrics import *
import torch.nn as nn
import numpy as np
import PIL.Image as pil
import torch.functional as F
from fastai.utils.mem import *
from pathlib import Path
import itertools

def channelify(mask):
    #bin mask shape 1,h,w
    zeroth = 1-mask
    return torch.cat((zeroth, mask), 0)

class OCTImageSegment(Image):
    "Support applying transforms to segmentation masks data in `px`."
    def lighting(self, func:LightingFunc, *args:Any, **kwargs:Any)->'Image': return self

    def refresh(self):
        self.sample_kwargs['mode'] = 'nearest'
        return super().refresh()

    @property
    def data(self)->TensorImage:
        "Return this image pixels as a `LongTensor`."
        #return channelify(self.px).float()
        return self.px.float()

    def show(self, ax:plt.Axes=None, figsize:tuple=(3,3), title:Optional[str]=None, hide_axis:bool=True,
        cmap:str='tab20', alpha:float=0.5, **kwargs):
        "Show the `ImageSegment` on `ax`."
        ax = show_image(self, ax=ax, hide_axis=hide_axis, cmap=cmap, figsize=figsize,
                        interpolation='nearest', alpha=alpha, vmin=0, **kwargs)
        if title: ax.set_title(title)

    def reconstruct(self, t:Tensor): return ImageSegment(t)

open_oct_mask = lambda x: OCTImageSegment(torch.gt(open_image(x).data[1], 0.1).unsqueeze(0).float())

def pct_seq(i,f,l):
    i,f = int(i*l), int(f*l)
    return range(i,f)

oct_pct_seq = partial(pct_seq, l=12011)

class SegLabelListCustom(SegmentationLabelList):
    def open(self, fn): return open_oct_mask(fn)

class SegItemListCustom(SegmentationItemList):
    _label_cls = SegLabelListCustom

def squash(s, axis = -1, epsilon = 1e-7):
    squared_norm = torch.sum(s*s, dim = axis)

    safe_norm = torch.sqrt(squared_norm + epsilon)

    squash_factor = squared_norm / (1. + squared_norm)

    #the unsqueeze here is very important!
    #for safe norm to be broadcasted appropriately
    unit_vector = torch.div(s, safe_norm.unsqueeze(-1))

    #for squash factor to be broadcasted appropriately
    return torch.mul(squash_factor.unsqueeze(-1), unit_vector)

class GPC(nn.Module):
    def __init__(self, data, n_m, n_d):
        super(GPC, self).__init__()
        self.c_in = data.c_in
        self.n_m, self.n_d = n_m, n_d
        #get intermediate number of maps
        i_m = int(self.c_in + ((self.n_m*self.n_d)-self.c_in)//2)

        self.c1 = conv_layer(self.c_in, i_m)
        self.c2 = conv_layer(i_m, int(self.n_m*self.n_d))
        self.f_resize = Lambda(lambda x: self.final_resizer(x))

    def final_resizer(self,x):
        xs = x.size()
        x = x.view([-1,self.n_m, self.n_d, xs[-1], xs[-1]])
        x = x.permute(0,1,3,4,2)
        x = squash(x)
        return x

    def forward(self,x):
        self.in_shape = x.size()
        x = self.c1(x)
        x = self.c2(x)
        x = self.final_resizer(x)
        self.out_shape = x.size()
        return x

class FCL(nn.Module):
    def __init__(self, data, prev_shape):
        super(FCL, self).__init__()
        self.data, self.prev_shape = data, prev_shape
        #get intermediate number of maps
        in_c = int(prev_shape[1] * prev_shape[4])

        self.c1 = t_conv_layer(in_c, in_c//2, stride=2)
        self.c2= t_conv_layer(in_c//2, data.c_out, stride=2)
        self.f_resize = Lambda(lambda x: self.pre_resizer(x))

    def pre_resizer(self,x):
        xs = x.size()
        x = x.permute(0,1,4,2,3)
        x = x.contiguous().view([-1,xs[1]*xs[4],xs[2],xs[3]])
        return x

    def forward(self,x):
        x = self.f_resize(x)
        x = self.c1(x)
        x = self.c2(x)
        self.out_shape = x.size()
        return x

def conv_layer(ni, nf, ks=3, stride=2, bn=True):
    layers = [nn.Conv2d(ni, nf, ks,padding=int(ks//2), stride=stride, bias=not bn),
             nn.ReLU()]
    if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))
    return nn.Sequential(*layers)

def t_conv_layer(ni, nf, ks=3, stride=2, bn=True):
    layers = [nn.ConvTranspose2d(ni, nf, ks,padding=int(ks//2), output_padding=1, stride=stride, bias=not bn),
             nn.ReLU()]
    if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))
    return nn.Sequential(*layers)

def get_usandstds(trainset):
    us=torch.tensor([0.,0.,0.])
    stds=torch.tensor([0.,0.,0.])

    for i,sample in tqdm.tqdm(enumerate(trainset)):
        im = sample[0].data
        us = us + im.view([3,-1]).mean(dim=-1)
        stds = stds + im.view([3,-1]).std(dim=-1)

    us = us/i
    stds=stds/i
    return us, stds

#get_usandstds(src.trainset)

def normalise_3c(x):
    xl = x.view([3,-1])
    u, s = xl.mean(dim=-1), xl.std(dim=-1)
    return (x-u[:,None,None])/s[:,None,None]

class Lambda(nn.Module):
    def __init__(self,func):
        super(Lambda, self).__init__()
        self.func=func
    def forward(self,x): return self.func(x)

class GaussianSmoothing(nn.Module):
    """
    Apply gaussian smoothing on a
    1d, 2d or 3d tensor. Filtering is performed seperately for each channel
    in the input using a depthwise convolution.
    Arguments:
        channels (int, sequence): Number of channels of the input tensors. Output will
            have this number of channels as well.
        kernel_size (int, sequence): Size of the gaussian kernel.
        sigma (float, sequence): Standard deviation of the gaussian kernel.
        dim (int, optional): The number of dimensions of the data.
            Default value is 2 (spatial).
    """
    def __init__(self, channels, kernel_size, sigma, dim=2):
        super(GaussianSmoothing, self).__init__()
        if isinstance(kernel_size, numbers.Number):
            kernel_size = [kernel_size] * dim
        if isinstance(sigma, numbers.Number):
            sigma = [sigma] * dim

        # The gaussian kernel is the product of the
        # gaussian function of each dimension.
        kernel = 1
        meshgrids = torch.meshgrid(
            [
                torch.arange(size, dtype=torch.float32)
                for size in kernel_size
            ]
        )
        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):
            mean = (size - 1) / 2
            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \
                      torch.exp(-((mgrid - mean) / std) ** 2 / 2)

        # Make sure sum of values in gaussian kernel equals 1.
        kernel = kernel / torch.sum(kernel)

        # Reshape to depthwise convolutional weight
        kernel = kernel.view(1, 1, *kernel.size())
        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))

        self.register_buffer('weight', kernel)
        self.groups = channels

        if dim == 1:
            self.conv = F.conv1d
        elif dim == 2:
            self.conv = F.conv2d
        elif dim == 3:
            self.conv = F.conv3d
        else:
            raise RuntimeError(
                'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)
            )

    def forward(self, input):
        """
        Apply gaussian filter to input.
        Arguments:
            input (torch.Tensor): Input to apply gaussian filter on.
        Returns:
            filtered (torch.Tensor): Filtered output.
        """
        return self.conv(input, weight=self.weight, groups=self.groups)

class CapsLayer(nn.Module):
    __allowed = ('o_maps', 'o_dims', 'stride', 'smooth')
    def __init__(self, data, prev_shape=None, down=True, **kwargs):
        super(CapsLayer, self).__init__()
        self.data = data
        self.prev_shape = prev_shape
        self.bs, self.i_maps, self.oldw, self.oldh, self.i_dims = prev_shape
        self.down=down

        #this is so we can add kwards o_dims and o_maps
        for k,v in kwargs.items():
            assert(k in self.__class__.__allowed)
            setattr(self, k, v)

        self.pre_conv_resize = Lambda(lambda x: self.pre_conv_resize(x))

        in_c, out_c = int(self.i_maps*self.i_dims), int(self.o_maps*self.o_dims)

        #now we sort out the conv layer and the output shape depending on whether up or down conv
        if self.down:
            self.c1 = conv_layer(in_c, out_c, stride=self.stride)
            self.neww, self.newh = int(self.oldw/self.stride), int(self.oldh/self.stride)
        elif not self.down:
            self.c1 = t_conv_layer(in_c, out_c, stride=self.stride)
            self.neww, self.newh = int(self.oldw*self.stride), int(self.oldh*self.stride)


        self.o_shape = [self.o_maps, int(self.neww*self.newh), self.i_maps]

        self.pre_route_resize = Lambda(lambda x: self.pre_route_resize(x))

        self.route = Routing(self.data, self.o_shape, n_iter=3)

        try: self.smooth==True
        except: self.smooth=False
        
        if self.smooth:
            self.smoother = GaussianSmoothing(channels = int(self.o_maps*self.o_dims), kernel_size=3, sigma=2)



    def pre_conv_resize(self, x):
        x = x.permute(0,1,4,2,3)
        x = x.contiguous().view(self.bs, int(self.i_maps*self.i_dims), self.oldw, self.oldh)
        self.b_s=x.size()
        #print('pre_conv_resize',self.b_s)
        return x

    def pre_route_resize(self, x):
        assert(len(self.b_s)==4)
        x = x.view(self.bs, self.o_maps, self.o_dims, -1)
        x = x.permute(0,1,3,2)
        self.c_s=x.size()
        #print('pre_route_resize',self.c_s)
        return x

    def smoothing(self, x):
        x = x.permute(0,1,4,2,3).reshape([self.bs, int(self.o_maps*self.o_dims), self.neww, self.newh])
        x = self.smoother(x)
        x=F.pad(x, (2,2,2,2), 'reflect')
        return x

    def forward(self,x):
        x=self.pre_conv_resize(x)
        x=self.c1(x)
        x=self.pre_route_resize(x)
        #print(x.size())
        x=self.route(x)
        x=x.view([self.bs, self.o_maps, self.neww, self.newh, self.o_dims])

        if self.smooth:
            self.smoothing(x)
        return x.view([self.bs, self.o_maps, self.o_dims, self.neww, self.newh]).permute(0,1,3,4,2)

class Routing(nn.Module):
    def __init__(self, data, in_shape, n_iter):
        super(Routing, self).__init__()
        self.n_iter = n_iter
        #we assume we want to route between same dimensions (e.g. 5 dim capsule vector to 5 dim capsule vector)
        #in_shape = [in_shape[0], in_shape[1], in_shape[0]]
        self.b = torch.zeros(in_shape)[None].to(data.device)

    def forward(self, x):
        self.c = torch.nn.functional.softmax(self.b, dim=-1)
        x = x[:,:,:,None,:]
        #print('1',x.size(),self.c.size())
        o = torch.mul(self.c[:,:,:,:,None], x)
        #print('2',o.size())
        o = torch.sum(o, dim=-2)
        #print('3',o.size())
        o = squash(o, axis=-1)
        c_b=self.c
        for i in range(self.n_iter):
            #print('a',c_b.size())
            c_b = c_b + torch.mul(o[:,:,:,None,:], x).sum(dim=-1)
            #print('b',c_b.size())
            c_b = torch.nn.functional.softmax(c_b, dim=-2)
            #print('c',c_b.size())
            o = torch.mul(x, c_b[:,:,:,:,None]).sum(-2)
            #print('d',o.size())
            o = squash(o, axis=-1)
            #print('e',o.size())
        self.c=c_b
        return o

class CapsSegNetwork(nn.Module):
    __allowed = ('o_maps', 'o_dims', 'stride', 'smooth') #change this to be relevant to capsegnetwork
    def __init__(self, data, maps, dims, c_out, smooth):
        super(CapsSegNetwork, self).__init__()
        assert len(maps)==len(dims), "Given maps and dims lists aren't the same size"

        self.data = data
        self.device = data.device
        self.maps = maps
        self.dims = dims
        self.c_out = c_out
        self.c_in = data.c_in
        self.layers = []
        self.sizes = []
        self.smooth = smooth

    def create_capsnet(self):
        maps, dims = self.maps, self.dims
        c_in = self.data.c_in
        dummy = (self.data.one_batch()[0]).cuda()
        self.sizes.append(dummy.size())
        with torch.no_grad():
            self.prim = GPC(self.data, maps[0], dims[0]).to(self.device)
            self.layers.append(self.prim)
            dummy = self.prim(dummy)
            self.sizes.append(self.prim.out_shape)
            #down branch
            for count, (m,d) in enumerate(zip(maps[1:-1], dims[1:-1])):
                layer_name = 'caps_layer_' + str(count)
                if count==0: prev_shape = self.prim.out_shape
                ith_layer = CapsLayer(self.data, prev_shape, stride=2, o_maps=m, o_dims=d, smooth=False).to(self.device)
                dummy = ith_layer(dummy)
                self.layers.append(ith_layer)
                setattr(self, layer_name, ith_layer)
                prev_shape=dummy.size()
                self.sizes.append(prev_shape)

            #bottom layer
            layer_name = 'caps_layer_bot'
            ith_layer = CapsLayer(self.data, prev_shape, stride=2, o_maps=maps[-1], o_dims=dims[-1], smooth=False).to(self.device)
            dummy = ith_layer(dummy)
            self.layers.append(ith_layer)
            setattr(self, layer_name, ith_layer)
            prev_shape=dummy.size()
            self.sizes.append(prev_shape)

            #upbranch, miss last element of maps and dims because we already made bottom layer
            for i, (m,d) in enumerate(zip(self.maps[:-1][::-1], self.dims[:-1][::-1])):
                count0 = count + 1 -i
                layer_name = 'caps_layer_' + str(count0)+'up'
                ith_layer = CapsLayer(self.data, prev_shape, down=False, stride=2, o_maps=m, o_dims=d, smooth=self.smooth).to(self.device)
                dummy = ith_layer(dummy)
                self.layers.append(ith_layer)
                setattr(self, layer_name, ith_layer)
                prev_shape=dummy.size()
                self.sizes.append(prev_shape)

            self.final = FCL(self.data, prev_shape).cuda()
            dummy = self.final(dummy)
            self.layers.append(self.final)
            self.sizes.append(dummy.size())
            self.soft = torch.nn.Softmax(dim=-1)
        return True

    def forward(self, x):
        x = self.prim(x)
        xp=x
        x = self.caps_layer_0(x)
        x0=x
        x = self.caps_layer_1(x)
        x1=x
        x = self.caps_layer_bot(x)
        x = self.caps_layer_2up(x)
        x = x.add_(x1)
        x = self.caps_layer_1up(x)
        x = x.add_(x0)
        x = self.caps_layer_0up(x)
        x=x.add_(xp)
        x = self.final(x)
        x = self.soft(x)
        return x


from fastai.callbacks import *

def channelify(mask):
    #bin mask shape 1,h,w
    zeroth = 1-mask
    return torch.cat((zeroth, mask), 1)

#@dataclass
class BinLabelCallback(LearnerCallback):
    def __init__(self, learner):
        self.learner=learner

    def on_batch_begin(self, last_input, last_target, **kwargs):
        new_target = channelify(last_target)
        return {'last_input': last_input, 'last_target': new_target}


def sens(c, l):
    n_targs=l.size()[0]
    c = c.argmax(dim=1).view(n_targs,-1).float()
    l=l.argmax(dim=1).view(n_targs, -1).float()
    inter = torch.sum(c*l, dim=(1))
    union = torch.sum(c, dim=(1)) + torch.sum(l) - inter
    return (inter/union).mean()

def spec(c, l):
    n_targs=l.size()[0]
    c = c.argmax(dim=1).view(n_targs,-1).float()
    l = l.argmax(dim=1).view(n_targs,-1).float()
    c=1-c
    l=1-l
    inter = torch.sum(c*l, dim=(1))
    union = torch.sum(c, dim=(1)) + torch.sum(l) - inter
    return (inter/union).mean()

def acc(c, l):
    n_targs=l.size()[0]
    c = c.argmax(dim=1).view(n_targs,-1)
    l = l.argmax(dim=1).view(n_targs,-1)
    c = torch.sum(torch.eq(c,l).float(),dim=1)
    return (c/l.size()[-1]).mean()

def mydice(c,l, iou:bool=False, eps:float=1e-8):
    "Dice coefficient metric for binary target. If iou=True, returns iou metric, classic for segmentation problems."
    n = l.shape[0]
    c = c.argmax(dim=1).view(n,-1)
    l = l.argmax(dim=1).view(n,-1)
    intersect = (c * l).sum().float()
    union = (c+l).sum().float()
    if not iou: return (2. * intersect / union if union > 0 else union.new([1.]).squeeze())
    else: return (intersect / (union-intersect+eps) if union > 0 else union.new([1.]).squeeze())

def softdice(c,l, iou:bool=False, eps:float=1e-8):
    "Dice coefficient metric for binary target. If iou=True, returns iou metric, classic for segmentation problems."
    n = l.shape[0]
    c = c[:,1,:,:].view(n,-1)
    l = l.argmax(dim=1).view(n,-1).float()
    intersect = (c * l).sum().float()
    union = (c+l).sum().float()
    if not iou: return (2. * intersect / union if union > 0 else union.new([1.]).squeeze())
    else: return (intersect / (union-intersect+eps) if union > 0 else union.new([1.]).squeeze())


class Dice_Loss(torch.nn.Module):
    """This is a custom Dice Similarity Coefficient loss function that we use
    to the accuracy of the segmentation. it is defined as ;
    DSC = 2 * (pred /intersect label) / (pred /union label) for the losss we use
    1- DSC so gradient descent leads to better outputs."""
    def __init__(self, weight=None, size_average=False):
        super(Dice_Loss, self).__init__()

    def forward(self, pred, label):
        label = label.float()
        smooth = 1.              #helps with backprop
        intersection = torch.sum(pred * label)
        union = torch.sum(pred) + torch.sum(label)
        loss = (2. * intersection + smooth) / (union + smooth)
        #return 1-loss because we want to minimise dissimilarity
        return 1 - (loss)

def one_group_selective_freeze(model, name_to):
    for name, child in model.named_children():
        if name == name_to: return model
        for param in child.parameters():
            param.requires_grad=False